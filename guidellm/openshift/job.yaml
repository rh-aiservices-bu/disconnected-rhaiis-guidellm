apiVersion: batch/v1
kind: Job
metadata:
  name: guidellm
  labels:
    app: guidellm
spec:
  template:
    metadata:
      labels:
        app: guidellm
    spec:
      restartPolicy: Never
      securityContext: {}
      containers:
      - name: guidellm
        image: quay.io/rh-aiservices-bu/guidellm:f1f8ca8
        env:
        - name: GUIDELLM_TARGET
          value: "http://llama-31-8b-instruct-w4a16-rhaiis:8000"
        - name: GUIDELLM_MODEL
          value: "meta-llama-3.1-8B-instruct-quantized.w4a16"
        - name: GUIDELLM_RATE_TYPE
          value: "sweep"
        - name: GUIDELLM_DATA
          value: "prompt_tokens=256,output_tokens=128"
        - name: GUIDELLM_MAX_REQUESTS
          value: "100"
        - name: GUIDELLM_MAX_SECONDS
          value: ""
        - name: GUIDELLM_OUTPUT_PATH
          value: "/results/results.json"
        - name: GUIDELLM_PROCESSOR
          value: "/mnt/tokenizer"
        resources:
          requests:
            memory: "8Gi"
            cpu: "500m"
          limits:
            memory: "8Gi"
            cpu: "1000m"
        volumeMounts:
        - name: results
          mountPath: /results
        - name: tokenizer-pvc
          mountPath: /mnt/tokenizer
      volumes:
      - name: results
        persistentVolumeClaim:
          claimName: guidellm-results
      - name: tokenizer-pvc
        persistentVolumeClaim:
          claimName: llama-31-8b-instruct-w4a16-tokenizer